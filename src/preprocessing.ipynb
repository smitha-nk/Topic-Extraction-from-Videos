{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7832f7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from typing import Tuple\n",
    "import gensim\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import re\n",
    "import import_ipynb\n",
    "from src.stopwords import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0be7e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_preprocessing(text: list, labels: list, dataset: str, dataset_type: str) \\\n",
    "        -> Tuple[list, list, list, list]:\n",
    "    \n",
    "    haspickledfile = \"no\"\n",
    "    prefix = dataset.lower()\n",
    "         \n",
    "    if dataset_type ==\"train\":\n",
    "        if Path(\"data/\"+prefix+\"_processed_train_text.pickle\").is_file():\n",
    "            \n",
    "            haspickledfile = \"yes\"\n",
    "            with open(\"data/\"+prefix+\"_processed_train_text.pickle\", \"rb\") as myFile:\n",
    "                processed_text = pickle.load(myFile)\n",
    "                \n",
    "            with open(\"data/\"+prefix+\"_processed_train_label.pickle\", \"rb\") as myFile:\n",
    "                processed_label = pickle.load(myFile)\n",
    "                \n",
    "            with open(\"data/\"+prefix+\"_train_vocabulary.pickle\", \"rb\") as myFile:\n",
    "                vocabulary = pickle.load(myFile)\n",
    "                \n",
    "            with open(\"data/\"+prefix+\"_train_raw_tokens.pickle\", \"rb\") as myFile:\n",
    "                raw_tokens = pickle.load(myFile)\n",
    "                    \n",
    "    if dataset_type ==\"test\":\n",
    "            \n",
    "        if Path(\"data/\"+prefix+\"_processed_test_text.pickle\").is_file():\n",
    "                \n",
    "            haspickledfile = \"yes\"\n",
    "            with open(\"data/\"+prefix+\"_processed_test_text.pickle\", \"rb\") as myFile:\n",
    "                processed_text = pickle.load(myFile)\n",
    "                \n",
    "            with open(\"data/\"+prefix+\"_processed_test_label.pickle\", \"rb\") as myFile:\n",
    "                processed_label = pickle.load(myFile)\n",
    "                \n",
    "            with open(\"data/\"+prefix+\"_test_vocabulary.pickle\", \"rb\") as myFile:\n",
    "                vocabulary = pickle.load(myFile)\n",
    "                \n",
    "            with open(\"data/\"+prefix+\"_test_raw_tokens.pickle\", \"rb\") as myFile:\n",
    "                raw_tokens = pickle.load(myFile)\n",
    "                    \n",
    "                \n",
    "    if haspickledfile ==\"no\":\n",
    "        \n",
    "        nltk.download('averaged_perceptron_tagger')\n",
    "        nltk.download('wordnet')\n",
    "        \n",
    "        stop_words = get_stopwords()\n",
    "                \n",
    "        if dataset == \"MUSE\":\n",
    "            remove_low_freq = False\n",
    "            do_just_nouns = True\n",
    "        else:\n",
    "            remove_low_freq = False\n",
    "            count_threshold = 30\n",
    "            do_just_nouns = False\n",
    "        \n",
    "        vocabulary = []\n",
    "        processed_text = []\n",
    "        processed_label = []\n",
    "        raw_tokens = []\n",
    "#         pre_processed_text = []\n",
    "    \n",
    "        for i, doc in enumerate(text):\n",
    "\n",
    "            doc = doc.lower()\n",
    "            tokens = word_tokenize(doc)\n",
    "        \n",
    "            cleaned_tokens = [w for w in tokens if w not in stop_words]\n",
    "            \n",
    "            cleaned_tokens = [re.sub('\\S*@\\S*\\s?', '', w) for w in cleaned_tokens]\n",
    "\n",
    "            # Remove new line characters\n",
    "            cleaned_tokens = [re.sub('\\s+', ' ', w) for w in cleaned_tokens]\n",
    "\n",
    "            # Remove distracting single quotes\n",
    "            cleaned_tokens = [re.sub(\"\\'\", \"\", w) for w in cleaned_tokens]\n",
    "            \n",
    "            # remove all tokens that are just digits\n",
    "            tokens = [w for w in tokens if w.isalpha()]\n",
    "\n",
    "            # remove all tokens that are < 3\n",
    "            tokens = [w for w in tokens if len(w) > 2]\n",
    "            \n",
    "            \n",
    "            if do_just_nouns:\n",
    "                #NN: Noun, singular or mass, NNS: Noun, plural, NNP: Proper noun, singular Phrase, NNPS: Proper noun, plural\n",
    "                cleaned_tokens = [w for (w, pos) in nltk.pos_tag(cleaned_tokens) if pos in ['NN', 'NNP', 'NNS', 'NNPS', 'NOUN']]\n",
    "                \n",
    "            cleaned_tokens = [WordNetLemmatizer().lemmatize(w) for w in cleaned_tokens]\n",
    "                \n",
    "\n",
    "            if len(cleaned_tokens) == 0:\n",
    "                continue\n",
    "            #print(\"tokens - \",tokens)\n",
    "            \n",
    "            \n",
    "\n",
    "            processed_text.append(cleaned_tokens)\n",
    "            processed_label.append(labels[i])\n",
    "        \n",
    "            vocabulary.extend(cleaned_tokens)\n",
    "            raw_tokens.append(tokens)\n",
    "            \n",
    "                  \n",
    "            #print(processed_text)\n",
    "        print(len(processed_text))\n",
    "        print(len(processed_label))\n",
    "        \n",
    "        if remove_low_freq:\n",
    "        # remove low-frequency terms\n",
    "\n",
    "            temp = []\n",
    "            for d in processed_text:\n",
    "                temp.extend(d)\n",
    "                counter = Counter(temp)\n",
    "\n",
    "            docs_threshold = []\n",
    "            labels_threshold = []\n",
    "            vocab_threshold = []\n",
    "        \n",
    "            for i, d in enumerate(processed_text):\n",
    "\n",
    "                d_threshold = [w for w in d if counter[w] > count_threshold]\n",
    "                if len(d_threshold) > 0:\n",
    "\n",
    "                    labels_threshold.append(processed_label[i])\n",
    "                    docs_threshold.append(d_threshold)\n",
    "                    vocab_threshold.extend(d_threshold)\n",
    "\n",
    "            print(\"vocab with out threshold len: \" + str(len(vocabulary)))\n",
    "            print(\"vocab threshold len: \" + str(len(vocab_threshold)))\n",
    "            processed_text = docs_threshold\n",
    "            vocabulary = vocab_threshold\n",
    "            processed_label = labels_threshold\n",
    "            \n",
    "        \n",
    "        vocabulary = sorted(list(set(vocabulary)))\n",
    "            \n",
    "        if dataset_type ==\"train\":\n",
    "            \n",
    "            with open(\"data/\"+prefix+\"_processed_train_text.pickle\", \"wb\") as myFile:\n",
    "                pickle.dump(processed_text, myFile)\n",
    "                \n",
    "            with open(\"data/\"+prefix+\"_processed_train_label.pickle\", \"wb\") as myFile:\n",
    "                pickle.dump(processed_label, myFile)\n",
    "                \n",
    "            with open(\"data/\"+prefix+\"_train_vocabulary.pickle\", \"wb\") as myFile:\n",
    "                pickle.dump(vocabulary, myFile)\n",
    "                \n",
    "            with open(\"data/\"+prefix+\"_train_raw_tokens.pickle\", \"wb\") as myFile:\n",
    "                pickle.dump(raw_tokens, myFile)\n",
    "                \n",
    "        elif dataset_type ==\"test\":\n",
    "            \n",
    "            with open(\"data/\"+prefix+\"_processed_test_text.pickle\", \"wb\") as myFile:\n",
    "                pickle.dump(processed_text, myFile)\n",
    "                \n",
    "            with open(\"data/\"+prefix+\"_processed_test_label.pickle\", \"wb\") as myFile:\n",
    "                pickle.dump(processed_label, myFile)\n",
    "                \n",
    "            with open(\"data/\"+prefix+\"_test_vocabulary.pickle\", \"wb\") as myFile:\n",
    "                pickle.dump(vocabulary, myFile)\n",
    "                \n",
    "            with open(\"data/\"+prefix+\"_test_raw_tokens.pickle\", \"wb\") as myFile:\n",
    "                pickle.dump(raw_tokens, myFile)\n",
    "\n",
    "    \n",
    "    print(len(processed_text))\n",
    "    print(len(processed_label))\n",
    "    assert len(processed_text) == len(processed_label)\n",
    "    return processed_text, processed_label, vocabulary, raw_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f704d229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    bigram = gensim.models.Phrases(texts, min_count=2, threshold=100) # higher threshold fewer phrases.\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return([bigram[doc] for doc in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e680acb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trigrams(texts):\n",
    "    trigram = gensim.models.Phrases(bigram[texts], threshold=100)  \n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824b2c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(texts):\n",
    "    train_sentences = [' '.join(text) for text in texts]\n",
    "    vect =TfidfVectorizer(stop_words=\"english\",max_features=100)\n",
    "    vect_text=vect.fit_transform(train_sentences)\n",
    "    \n",
    "    idf=vect.idf_\n",
    "    dd=dict(zip(vect.get_feature_names(), idf))\n",
    "    l=sorted(dd, key=(dd).get)\n",
    "    \n",
    "    return vect, vect_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
