{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6879e6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33bc7781",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_data_path='data/processed_tasks/c2_muse_topic'\n",
    "transcription_path='data/processed_tasks/c2_muse_topic/transcription_segments'\n",
    "root_path = 'C:/Users/Dell/Desktop/Imarticus-learning/Capstone/DL-capstone/NLP/graphbasedTM/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ff60184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition() -> (dict, collections.defaultdict):\n",
    "    \"\"\"\n",
    "    get_partition fetches information on what sample ID is for training/developing/testing\n",
    "\n",
    "    :param task_data_path:\n",
    "    :param path: csv file that maps each sample ID to a train/devel/test\n",
    "    :return: dicts with mappings between the sample IDs and the proposal\n",
    "    \"\"\"\n",
    "    \n",
    "    # any label to collect filenames safely\n",
    "    filepath = root_path + task_data_path + '/label_segments/arousal/*.csv'\n",
    "    names = glob.glob(filepath)\n",
    "    sample_ids = []\n",
    "    \n",
    "    for n in names:\n",
    "        name_split = n.split(os.path.sep)[-1].split('.')[0]\n",
    "        sample_ids.append(int(name_split))\n",
    "    sample_ids = set(sample_ids)\n",
    "\n",
    "    df = pd.read_csv(root_path + \"data/processed_tasks/metadata/partition.csv\", delimiter=\",\")\n",
    "    data = df[[\"Id\", \"Proposal\"]].values\n",
    "\n",
    "\n",
    "    partition_to_id = collections.defaultdict(set)\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        sample_id = int(data[i, 0])\n",
    "        partition = data[i, 1]\n",
    "\n",
    "        if sample_id not in sample_ids:\n",
    "            continue\n",
    "\n",
    "        partition_to_id[partition].add(sample_id)\n",
    "\n",
    "    return partition_to_id\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9cf37c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_classification_classes(label_file):\n",
    "    \"\"\"\n",
    "    read_classification_classes is used to extract the class_ids from the label file\n",
    "\n",
    "    :param label_file: path to csv file\n",
    "    :return: list of class ids\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(label_file, delimiter=\",\", usecols=['class_id'])\n",
    "    \n",
    "    label_list = df['class_id'].tolist()\n",
    "    return label_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "181bca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_trans_files(elem):\n",
    "#     print(\"inside sort_trans_files\", elem)\n",
    "    \"\"\"\n",
    "    sort_trans_files is used to calculate a key with which the transcriptions files are sorted\n",
    "\n",
    "    :param elem: a file name\n",
    "    :return: file weight used in sorting\n",
    "    \"\"\"\n",
    "    return int(elem.split('_')[-1].split('.')[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e32b24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data() -> dict:\n",
    "    \"\"\"\n",
    "    prepare_data creates a dict for the segment-level transcripts and their topic label\n",
    "    :param task_data_path:\n",
    "    :param transcription_path:\n",
    "    :return: dict that consists of transcripts and their topic label\n",
    "    \"\"\"\n",
    "    # Reading transcriptions on SEGMENT-level, sep. in train, develop, test of the official challenge\n",
    "   \n",
    "    partition_to_id = get_partition()\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    # training with test labels available\n",
    "    for partition in tqdm(partition_to_id.keys()):\n",
    "        text = []\n",
    "        y = []\n",
    "\n",
    "        for sample_id in tqdm(sorted(partition_to_id[partition])):\n",
    "            transcription_files = root_path + transcription_path+'/'+str(sample_id)+'/*.' + 'csv'\n",
    "            filenames = glob.glob(transcription_files)\n",
    "\n",
    "            for file in sorted(filenames, key=sort_trans_files):\n",
    "                df = pd.read_csv(file, delimiter=',')\n",
    "                words = df['word'].tolist()\n",
    "                text.append(\" \".join(words))\n",
    "\n",
    "            # extracting labels available\n",
    "            label_file = root_path + task_data_path+'/label_segments/topic/'+str(sample_id) + \".csv\"\n",
    "            label_list = read_classification_classes(label_file)\n",
    "\n",
    "            for i in label_list:\n",
    "                y.append(i)\n",
    "\n",
    "        data[partition] = {'text': text, 'labels': y}\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "acc82777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_data(text: list, labels: list) -> (list, list):\n",
    "    \"\"\"\n",
    "    filter_dataset returns a set of segments without the very short segments,\n",
    "    and their respective labels\n",
    "\n",
    "    :param data: segment data set\n",
    "    :param data_labels: set of data labels\n",
    "    :return:\n",
    "        - filtered_data - segment data set without the short segments\n",
    "        - filtered_data_labels -  labels of the returning segment data set\n",
    "    \"\"\"\n",
    "    new_data, new_data_labels = zip(*((segment, label) for segment, label\n",
    "                                      in zip(text, labels)\n",
    "                                      if len([w for w in segment.split() if w.isalpha()]) > 2))\n",
    "\n",
    "    return new_data, new_data_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f937de70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_set: str, get_test_data: bool) \\\n",
    "        -> Tuple[list, list, list, list]:\n",
    "    \"\"\"\n",
    "    get_data collects the data and test_data\n",
    "\n",
    "    :param data_set: name of the data set (MUSE or CRR)\n",
    "    :param get_test_data: getting test data flag\n",
    "    :param task_data_path:  path to data task\n",
    "    :param transcription_path: path to transcription\n",
    "\n",
    "    :return:\n",
    "        - training data -\n",
    "        - training labels -\n",
    "        - testing data -\n",
    "        - testing labels -\n",
    "    \"\"\"\n",
    "    \n",
    "    if data_set == \"CRR\":\n",
    "        # using the Citysearch Restaurant Reviews corpus\n",
    "        data, test_data = ([], [])\n",
    "        with open(\"data/restaurant/test.txt\") as f:\n",
    "            for line in f:\n",
    "                if \"\\n\" != line:\n",
    "                    test_data.append(line.replace(\"\\n\", \"\"))\n",
    "\n",
    "        with open(\"data/restaurant/train.txt\") as f:\n",
    "            for line in f:\n",
    "                if \"\\n\" != line:\n",
    "                    data.append(line.replace(\"\\n\", \"\"))\n",
    "\n",
    "        test_labels = [-1 for _ in range(len(test_data))]\n",
    "        labels = [-1 for _ in range(len(data))]\n",
    "\n",
    "        return data, labels, test_data, test_labels\n",
    "    \n",
    "    if data_set == \"MUSE\":\n",
    "        \n",
    "        if Path(\"data/train_text.pickle\").is_file():\n",
    "            with open(\"data/train_text.pickle\", \"rb\") as myFile:\n",
    "                train_text = pickle.load(myFile)\n",
    "                \n",
    "            with open(\"data/train_label.pickle\", \"rb\") as myFile:\n",
    "                train_label = pickle.load(myFile)\n",
    "                \n",
    "            if get_test_data:\n",
    "                \n",
    "                with open(\"data/test_text.pickle\", \"rb\") as myFile:\n",
    "                    test_text = pickle.load(myFile)\n",
    "                    \n",
    "                with open(\"data/test_label.pickle\", \"rb\") as myFile:\n",
    "                    test_label = pickle.load(myFile)\n",
    "                    \n",
    "            else:\n",
    "                test_text = None\n",
    "                test_label = None\n",
    "                \n",
    "        else: \n",
    "            data_twain = prepare_data()\n",
    "            \n",
    "            train_text = data_twain['train']['text']\n",
    "            train_text.extend(data_twain['devel']['text'])\n",
    "            \n",
    "            train_label = data_twain['train']['labels']\n",
    "            train_label.extend(data_twain['devel']['labels'])\n",
    "            \n",
    "            with open(\"data/train_text.pickle\", \"wb\") as myFile:\n",
    "                pickle.dump(train_text, myFile)\n",
    "                \n",
    "            with open(\"data/train_label.pickle\", \"wb\") as myFile:\n",
    "                pickle.dump(train_label, myFile)\n",
    "                \n",
    "            if get_test_data:\n",
    "                test_text = data_twain['test']['text']\n",
    "                test_label = data_twain['test']['labels']\n",
    "                \n",
    "                with open(\"data/test_text.pickle\", \"wb\") as myFile:\n",
    "                    pickle.dump(test_text, myFile)\n",
    "                    \n",
    "                with open(\"data/test_label.pickle\", \"wb\") as myFile:\n",
    "                    pickle.dump(test_label, myFile)\n",
    "                    \n",
    "            else:\n",
    "                test_text = None\n",
    "                test_label = None\n",
    "            \n",
    "\n",
    "    X_train, Y_train = zip_data(train_text, train_label)\n",
    "\n",
    "    if get_test_data:\n",
    "        X_test, Y_test = zip_data(test_text, test_label)\n",
    "    else:\n",
    "        X_test, Y_test = (None, None)\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc62f9be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1589217b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
